{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction of Feature selection\n",
    "\n",
    "In previous tutorials, we have seen how to create feature, transform raw features into model-usable features (e.g. encoding), and engineer feature to improve model accuracy. We also need to decide which feature we will use, and why (feature importance).\n",
    "\n",
    "In this section, we will talk about it. The origin doc is from the book [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/)\n",
    "\n",
    "## 1. Important Concepts\n",
    "\n",
    "Before we start, let's learn some basic concepts:\n",
    "- Model Bias and Variance\n",
    "- over-fitting, under-fitting\n",
    "- ROC curve\n",
    "\n",
    "### 1.1 Bias\n",
    "\n",
    "**Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. It represents `the difference between the expected prediction of the model and the true target values`.\n",
    "Every algorithm starts with some level of bias, because bias results from [assumptions in the model that make the target function easier to learn](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/). A high level of bias can lead to underfitting, which occurs when the algorithm is unable to capture relevant relations between features and target outputs. A high bias model typically includes more assumptions about the target function or end result. A low bias model incorporates fewer assumptions about the target function.\n",
    "High bias indicates that the model is too simplistic and fails to capture the underlying patterns in the data. This can lead to underfitting, where the model's performance is poor on both training and test data.\n",
    "\n",
    "**A linear algorithm often makes more assumptions, which make them learn fast. But at the same time, they have high bias**. The bias is introduced by approximating a real-life problem(maybe complicated) into a linear problem. Though the linear algorithm can introduce bias, it also makes their output easier to understand. The simpler the algorithm, the more bias it has likely introduced. In contrast, nonlinear algorithms often have low bias.\n",
    "\n",
    "\n",
    "### 1.2 Variance\n",
    "\n",
    "[Variance indicates how much the estimate of the target function will alter if different training data were used](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/). In other words, variance describes how much a random variable differs from its expected value. Variance is based on a single training set. Variance measures the inconsistency of different predictions using different training sets  itâ€™s not a measure of overall accuracy.\n",
    "\n",
    "Variance can lead to overfitting, in which small fluctuations in the training set are magnified. A model with high-level variance may reflect random noise in the training data set instead of the target function. The model should be able to identify the underlying connections between the input data and variables of the output.\n",
    "\n",
    "A model with low variance means sampled data is close to where the model predicted it would be. A model with high variance will result in significant changes to the projections of the target function.\n",
    "\n",
    "Machine learning algorithms with low variance include linear regression, logistics regression, and linear discriminant analysis. Those with high variance include decision trees, support vector machines and k-nearest neighbors.\n",
    "\n",
    "### 1.3 Over-fitting\n",
    "\n",
    "**Over-fitting** is the situation where a model fits very well to the `current training data` but fails when predicting real world data. It typically occurs when the model has relied too heavily on patterns and trends in the training data set. And the training dataset do not represent all patterns and trends in the real world. Signs of overfitting include:\n",
    "\n",
    "- **High Training Performance**: The model achieves a very low error rate or high accuracy on the training data.\n",
    "- **Poor Test Performance**: The model's performance on new, unseen data (test data or validation data) is significantly worse than its performance on the training data.\n",
    "- **High Variance**: Overfitting is often associated with high variance. Variance refers to the model's sensitivity to small fluctuations in the training data. A high-variance model fits the training data too closely, including its noise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 Under-fitting\n",
    "\n",
    "**Under-fitting** is the situation where a data model is `unable to capture the relationship between the predictors(feature columns) and outcomes(target label) accurately`. A such model will generate a high error rate on both the training set and unseen data, Because the model fails to capture important patterns, relationships, and trends in the data. Signs of underfitting include:\n",
    "   - **Low Training Performance**: The model struggles to fit the training data and achieves a low accuracy or error rate.\n",
    "   - **Low Test Performance**: The model's performance on new, unseen data (test data or validation data) is also poor, indicating that it is not generalizing well.\n",
    "   - **High Bias**: Underfitting is often associated with high bias. Bias refers to the error due to overly simplistic assumptions in the learning algorithm. High bias means the model is not able to capture the complexity of the data.\n",
    "\n",
    "\n",
    "To address underfitting, you can try the following steps:\n",
    "\n",
    "- **Increase Model Complexity**: Use a more complex model that can capture intricate relationships in the data. For example, move from a linear model to a decision tree, random forest, or neural network.\n",
    "- **Feature Engineering**: Improve the quality of input features by transforming or creating new features that may better represent the underlying patterns.\n",
    "- **Increase Training Data**: More data can help the model learn better and generalize more effectively.\n",
    "- **Reduce Regularization**: If you're using regularized models (like Lasso or Ridge regression), reducing the strength of regularization can help prevent excessive simplification.\n",
    "- **Hyperparameter Tuning**: Adjust hyperparameters like learning rate, depth of trees, or number of layers in a neural network to find a better balance between model complexity and fit.\n",
    "- **Ensemble Methods**: Combine multiple models to improve overall performance. Ensemble methods like Random Forest or Gradient Boosting can be particularly helpful in addressing underfitting.\n",
    "\n",
    "\n",
    "> It's important to find the right balance between underfitting and overfitting.\n",
    "\n",
    "Below figure shows an example of bias vs variant and under-fitting vs over-fitting\n",
    "\n",
    "![Figure1](../../img/fs_intro-var-bias-orig-1.png)\n",
    "\n",
    "The figure contains a single predictor and outcome where their relationship is nonlinear. The right-hand panel (b) shows two model fits. First, a simple three-point moving average is used (in green). This trend line is bumpy but does a good job of tracking the nonlinear trend in the data. The purple line shows the results of a standard linear regression model that includes a term for the predictor value and a term for the square of the predictor value. Linear regression is a linear in the model parameters and adding polynomial terms to the model can be effective way of allowing the model to identify nonlinear patterns. Since the data points start low on the y-axis, reach an apex near a predictor value of 0.3 then decrease, a quadratic regression model would be a reasonable first attempt at modeling these data. This model is very smooth (showing low variance) but does not do a very good job of fitting the nonlinear trend seen in the data (i.e., high bias).\n",
    "\n",
    "### 1.5 ROC curve\n",
    "\n",
    "A Receiver Operating Characteristic (ROC) curve is a graphical representation that illustrates the performance of a binary classification model at different classification thresholds. It's a tool commonly used in machine learning and statistics to assess the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) as the decision threshold for classification is varied.\n",
    "\n",
    "\n",
    "The ROC curve provides a visual way to understand the performance of a binary classification model and its ability to discriminate between the positive and negative classes across various threshold choices. It's a valuable tool for model evaluation, especially when you want to assess the model's performance under different trade-offs between sensitivity and specificity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
