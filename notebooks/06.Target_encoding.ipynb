{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 06. Target encoding\n",
    "In previous chapter, we've seen how to engineer numerical features. In this chapter, we will learn **target encoding**, which is instead meant for categorical features. It's a method of encoding categories as numbers, like **one-hot** or **label encoding**, with the difference that it also uses the target to create the encoding. This makes it what we call a **supervised feature engineering technique**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path=\"../data/autos.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "autos=pd.read_csv(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   symboling         make fuel_type aspiration  num_of_doors   body_style  \\\n0          3  alfa-romero       gas        std             2  convertible   \n1          3  alfa-romero       gas        std             2  convertible   \n2          1  alfa-romero       gas        std             2    hatchback   \n3          2         audi       gas        std             4        sedan   \n4          2         audi       gas        std             4        sedan   \n\n  drive_wheels engine_location  wheel_base  length  ...  engine_size  \\\n0          rwd           front        88.6   168.8  ...          130   \n1          rwd           front        88.6   168.8  ...          130   \n2          rwd           front        94.5   171.2  ...          152   \n3          fwd           front        99.8   176.6  ...          109   \n4          4wd           front        99.4   176.6  ...          136   \n\n   fuel_system  bore stroke  compression_ratio  horsepower peak_rpm  city_mpg  \\\n0         mpfi  3.47   2.68                  9         111     5000        21   \n1         mpfi  3.47   2.68                  9         111     5000        21   \n2         mpfi  2.68   3.47                  9         154     5000        19   \n3         mpfi  3.19   3.40                 10         102     5500        24   \n4         mpfi  3.19   3.40                  8         115     5500        18   \n\n   highway_mpg  price  \n0           27  13495  \n1           27  16500  \n2           26  16500  \n3           30  13950  \n4           22  17450  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symboling</th>\n      <th>make</th>\n      <th>fuel_type</th>\n      <th>aspiration</th>\n      <th>num_of_doors</th>\n      <th>body_style</th>\n      <th>drive_wheels</th>\n      <th>engine_location</th>\n      <th>wheel_base</th>\n      <th>length</th>\n      <th>...</th>\n      <th>engine_size</th>\n      <th>fuel_system</th>\n      <th>bore</th>\n      <th>stroke</th>\n      <th>compression_ratio</th>\n      <th>horsepower</th>\n      <th>peak_rpm</th>\n      <th>city_mpg</th>\n      <th>highway_mpg</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>alfa-romero</td>\n      <td>gas</td>\n      <td>std</td>\n      <td>2</td>\n      <td>convertible</td>\n      <td>rwd</td>\n      <td>front</td>\n      <td>88.6</td>\n      <td>168.8</td>\n      <td>...</td>\n      <td>130</td>\n      <td>mpfi</td>\n      <td>3.47</td>\n      <td>2.68</td>\n      <td>9</td>\n      <td>111</td>\n      <td>5000</td>\n      <td>21</td>\n      <td>27</td>\n      <td>13495</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>alfa-romero</td>\n      <td>gas</td>\n      <td>std</td>\n      <td>2</td>\n      <td>convertible</td>\n      <td>rwd</td>\n      <td>front</td>\n      <td>88.6</td>\n      <td>168.8</td>\n      <td>...</td>\n      <td>130</td>\n      <td>mpfi</td>\n      <td>3.47</td>\n      <td>2.68</td>\n      <td>9</td>\n      <td>111</td>\n      <td>5000</td>\n      <td>21</td>\n      <td>27</td>\n      <td>16500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>alfa-romero</td>\n      <td>gas</td>\n      <td>std</td>\n      <td>2</td>\n      <td>hatchback</td>\n      <td>rwd</td>\n      <td>front</td>\n      <td>94.5</td>\n      <td>171.2</td>\n      <td>...</td>\n      <td>152</td>\n      <td>mpfi</td>\n      <td>2.68</td>\n      <td>3.47</td>\n      <td>9</td>\n      <td>154</td>\n      <td>5000</td>\n      <td>19</td>\n      <td>26</td>\n      <td>16500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>audi</td>\n      <td>gas</td>\n      <td>std</td>\n      <td>4</td>\n      <td>sedan</td>\n      <td>fwd</td>\n      <td>front</td>\n      <td>99.8</td>\n      <td>176.6</td>\n      <td>...</td>\n      <td>109</td>\n      <td>mpfi</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>10</td>\n      <td>102</td>\n      <td>5500</td>\n      <td>24</td>\n      <td>30</td>\n      <td>13950</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>audi</td>\n      <td>gas</td>\n      <td>std</td>\n      <td>4</td>\n      <td>sedan</td>\n      <td>4wd</td>\n      <td>front</td>\n      <td>99.4</td>\n      <td>176.6</td>\n      <td>...</td>\n      <td>136</td>\n      <td>mpfi</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>8</td>\n      <td>115</td>\n      <td>5500</td>\n      <td>18</td>\n      <td>22</td>\n      <td>17450</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 25 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Target Encoding\n",
    "\n",
    "A target encoding is any kind of **encoding that replaces a feature's categories with some number derived from the target**.\n",
    "\n",
    "A simple and effective version is to apply a group aggregation from chapter 3, like the mean. Using the Automobiles dataset, this computes the average price of each vehicle's make:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          make  price  make_encoded\n0  alfa-romero  13495  15498.333333\n1  alfa-romero  16500  15498.333333\n2  alfa-romero  16500  15498.333333\n3         audi  13950  17859.166667\n4         audi  17450  17859.166667\n5         audi  15250  17859.166667\n6         audi  17710  17859.166667\n7         audi  18920  17859.166667\n8         audi  23875  17859.166667\n9          bmw  16430  26118.750000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>make</th>\n      <th>price</th>\n      <th>make_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>alfa-romero</td>\n      <td>13495</td>\n      <td>15498.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>alfa-romero</td>\n      <td>16500</td>\n      <td>15498.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>alfa-romero</td>\n      <td>16500</td>\n      <td>15498.333333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>audi</td>\n      <td>13950</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>audi</td>\n      <td>17450</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>audi</td>\n      <td>15250</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>audi</td>\n      <td>17710</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>audi</td>\n      <td>18920</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>audi</td>\n      <td>23875</td>\n      <td>17859.166667</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>bmw</td>\n      <td>16430</td>\n      <td>26118.750000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos[\"make_encoded\"] = autos.groupby(\"make\")[\"price\"].transform(\"mean\")\n",
    "\n",
    "autos[[\"make\", \"price\", \"make_encoded\"]].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This kind of target encoding is sometimes called a **mean encoding**. Applied to a binary target, it's also called **bin counting**. (Other names you might come across include: `likelihood encoding, impact encoding, and leave-one-out encoding`.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Smoothing\n",
    "\n",
    "An encoding like this presents a couple of problems, however.\n",
    "\n",
    "1. **unknown categories**: Target encodings create a special risk of overfitting, which means they need to be trained on an independent \"encoding\" split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.\n",
    "\n",
    "2. **rare categories**: When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the `mercurcy` make only occurs once. The \"mean\" price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely.\n",
    "\n",
    "A solution to these problems is to add **smoothing**. The idea is to blend the `in-category average` with the `overall average`. Rare categories get less weight on their category average, while missing categories just get the overall average.\n",
    "\n",
    "In pseudocode:\n",
    "\n",
    "```text\n",
    "encoding = weight * in_category + (1 - weight) * overall\n",
    "```\n",
    "\n",
    "where weight is a value between 0 and 1 calculated from the category frequency.\n",
    "\n",
    "An easy way to determine the value for weight is to compute an **m-estimate**:\n",
    "\n",
    "```text\n",
    "weight = n / (n + m)\n",
    "\n",
    "```\n",
    "where `n` is the total number of times that category occurs in the data. The parameter `m` determines the \"smoothing factor\". Larger values of m put more weight on the overall estimate.\n",
    "\n",
    "![06.TE_weight_calc.png](../img/06.TE_weight_calc.png)\n",
    "\n",
    "In the Automobiles dataset there are three cars with the make chevrolet. If you chose m=2.0, then the chevrolet category would be encoded with 60% of the average Chevrolet price plus 40% of the overall average price.\n",
    "\n",
    "```text\n",
    "chevrolet = 0.6 * 6000.00 + 0.4 * 13285.03\n",
    "```\n",
    "\n",
    "### Tips on m value\n",
    "\n",
    "When choosing a value for m, consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for m; if the average price for each make were relatively stable, a smaller value could be okay.\n",
    "\n",
    "\n",
    "## 6.3  Use Cases for Target Encoding\n",
    "\n",
    "Target encoding is great for:\n",
    "1. **High-cardinality features**: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\n",
    "2. **Domain-motivated features**: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}